{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.mrnetdata import *\n",
    "from lib.plot import *\n",
    "from lib.MLMethod import *\n",
    "from lib.kneedata import *\n",
    "from lib.evaluator import *\n",
    "from lib.shapes import *\n",
    "from lib.vggModel import *\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.scans[:][:][:][70-5:70+5:6]\n",
    "with open(\"pickled/mrnet_abnormal_train.pck\",'rb') as fp:   \n",
    "    data = pickle.load(fp)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.52 0.44 0.5  0.54 0.56 0.48 0.46 0.52 0.5  0.52 0.52 0.46 0.44 0.54\n",
      " 0.52 0.48 0.48 0.52 0.5  0.46 0.46 0.48 0.48 0.52 0.52 0.52 0.52 0.52\n",
      " 0.52 0.5  0.52 0.52 0.54 0.54 0.46 0.56 0.54 0.46 0.54 0.46 0.44 0.56\n",
      " 0.5  0.52 0.54 0.5  0.48 0.54 0.54 0.46 0.46 0.52 0.52 0.48 0.54 0.5\n",
      " 0.52 0.44 0.5  0.52 0.52 0.54 0.46 0.52 0.5  0.56 0.5  0.5  0.54 0.46\n",
      " 0.46 0.52 0.48 0.5  0.48 0.56 0.54 0.46 0.5  0.54 0.5  0.52 0.44 0.52\n",
      " 0.54 0.52 0.46 0.5  0.5  0.52 0.52 0.46 0.44 0.44 0.56 0.5  0.5  0.5\n",
      " 0.52 0.52]\n",
      "[0.62 0.42 0.44 0.64 0.4  0.38 0.4  0.62 0.62 0.48 0.62 0.5  0.42 0.64\n",
      " 0.64 0.62 0.54 0.6  0.58 0.5  0.62 0.62 0.6  0.4  0.64 0.62 0.64 0.62\n",
      " 0.54 0.64 0.42 0.62 0.62 0.64 0.64 0.68 0.62 0.4  0.44 0.4  0.56 0.44\n",
      " 0.52 0.64 0.62 0.4  0.48 0.62 0.58 0.54 0.54 0.42 0.54 0.62 0.64 0.48\n",
      " 0.6  0.6  0.48 0.56 0.64 0.62 0.42 0.62 0.64 0.64 0.44 0.48 0.5  0.42\n",
      " 0.44 0.42 0.62 0.52 0.58 0.58 0.6  0.66 0.48 0.64 0.62 0.4  0.56 0.5\n",
      " 0.66 0.62 0.42 0.62 0.62 0.4  0.62 0.44 0.62 0.48 0.44 0.6  0.6  0.64\n",
      " 0.66 0.48]\n",
      "[0.6  0.42 0.46 0.62 0.42 0.44 0.42 0.6  0.58 0.56 0.6  0.42 0.44 0.46\n",
      " 0.6  0.6  0.58 0.58 0.46 0.46 0.46 0.58 0.6  0.44 0.62 0.6  0.62 0.6\n",
      " 0.6  0.62 0.42 0.58 0.6  0.58 0.6  0.54 0.6  0.42 0.48 0.42 0.46 0.5\n",
      " 0.4  0.62 0.6  0.42 0.6  0.56 0.46 0.46 0.62 0.46 0.46 0.58 0.6  0.58\n",
      " 0.58 0.48 0.48 0.54 0.6  0.56 0.46 0.6  0.58 0.6  0.48 0.44 0.62 0.42\n",
      " 0.48 0.46 0.44 0.58 0.52 0.6  0.48 0.62 0.4  0.6  0.6  0.48 0.6  0.54\n",
      " 0.62 0.58 0.42 0.58 0.6  0.46 0.48 0.46 0.58 0.46 0.54 0.58 0.58 0.6\n",
      " 0.62 0.58]\n",
      "[0.5  0.46 0.44 0.6  0.44 0.42 0.48 0.48 0.58 0.44 0.48 0.6  0.52 0.6\n",
      " 0.6  0.56 0.42 0.44 0.56 0.56 0.6  0.58 0.5  0.42 0.48 0.54 0.58 0.42\n",
      " 0.44 0.52 0.56 0.54 0.58 0.56 0.56 0.6  0.58 0.44 0.44 0.46 0.54 0.44\n",
      " 0.58 0.52 0.54 0.5  0.42 0.56 0.56 0.58 0.44 0.44 0.58 0.56 0.58 0.42\n",
      " 0.48 0.56 0.44 0.54 0.58 0.52 0.46 0.44 0.48 0.6  0.44 0.46 0.44 0.52\n",
      " 0.52 0.44 0.58 0.42 0.54 0.5  0.56 0.58 0.54 0.58 0.44 0.44 0.5  0.46\n",
      " 0.58 0.52 0.56 0.58 0.56 0.44 0.58 0.46 0.56 0.5  0.44 0.42 0.42 0.52\n",
      " 0.58 0.42]\n",
      "[0.62 0.34 0.54 0.6  0.4  0.44 0.36 0.62 0.64 0.56 0.64 0.36 0.36 0.54\n",
      " 0.64 0.54 0.52 0.62 0.54 0.4  0.46 0.62 0.36 0.54 0.66 0.52 0.54 0.64\n",
      " 0.54 0.66 0.4  0.62 0.52 0.52 0.36 0.36 0.56 0.36 0.54 0.36 0.48 0.46\n",
      " 0.56 0.66 0.6  0.4  0.62 0.58 0.52 0.36 0.4  0.54 0.36 0.64 0.58 0.6\n",
      " 0.6  0.34 0.48 0.36 0.54 0.6  0.36 0.62 0.64 0.56 0.48 0.6  0.62 0.36\n",
      " 0.36 0.48 0.44 0.56 0.6  0.36 0.52 0.52 0.36 0.64 0.62 0.5  0.48 0.54\n",
      " 0.46 0.64 0.36 0.58 0.62 0.5  0.48 0.36 0.38 0.38 0.54 0.64 0.64 0.6\n",
      " 0.46 0.54]\n",
      "[0.56 0.56 0.44 0.58 0.48 0.44 0.46 0.5  0.44 0.46 0.56 0.46 0.48 0.46\n",
      " 0.46 0.46 0.56 0.46 0.46 0.54 0.44 0.46 0.46 0.46 0.58 0.52 0.46 0.56\n",
      " 0.48 0.58 0.46 0.42 0.46 0.42 0.42 0.46 0.56 0.46 0.48 0.44 0.44 0.58\n",
      " 0.44 0.58 0.42 0.44 0.46 0.44 0.44 0.46 0.56 0.54 0.46 0.46 0.48 0.54\n",
      " 0.44 0.44 0.56 0.48 0.48 0.56 0.42 0.56 0.56 0.46 0.44 0.44 0.5  0.52\n",
      " 0.48 0.44 0.44 0.56 0.56 0.48 0.46 0.58 0.4  0.58 0.56 0.5  0.56 0.46\n",
      " 0.48 0.56 0.5  0.56 0.46 0.48 0.44 0.46 0.46 0.54 0.56 0.54 0.46 0.58\n",
      " 0.46 0.56]\n",
      "[0.46 0.5  0.48 0.52 0.46 0.46 0.54 0.46 0.5  0.48 0.46 0.52 0.52 0.52\n",
      " 0.52 0.5  0.46 0.48 0.52 0.52 0.52 0.5  0.5  0.46 0.48 0.52 0.52 0.46\n",
      " 0.48 0.48 0.52 0.48 0.5  0.52 0.54 0.52 0.5  0.48 0.48 0.5  0.52 0.48\n",
      " 0.5  0.48 0.52 0.54 0.46 0.52 0.52 0.54 0.48 0.48 0.54 0.52 0.54 0.46\n",
      " 0.46 0.5  0.46 0.52 0.54 0.48 0.48 0.46 0.44 0.52 0.46 0.48 0.48 0.52\n",
      " 0.46 0.48 0.5  0.46 0.52 0.48 0.54 0.56 0.5  0.54 0.46 0.48 0.48 0.48\n",
      " 0.54 0.44 0.54 0.5  0.5  0.48 0.5  0.48 0.52 0.5  0.48 0.46 0.46 0.48\n",
      " 0.54 0.46]\n",
      "[0.54 0.54 0.44 0.62 0.42 0.42 0.6  0.6  0.54 0.42 0.6  0.54 0.58 0.46\n",
      " 0.42 0.6  0.56 0.42 0.58 0.62 0.58 0.56 0.54 0.4  0.56 0.6  0.42 0.38\n",
      " 0.56 0.62 0.4  0.58 0.6  0.56 0.58 0.4  0.6  0.62 0.46 0.58 0.56 0.46\n",
      " 0.4  0.56 0.56 0.4  0.56 0.56 0.56 0.58 0.62 0.46 0.4  0.56 0.42 0.42\n",
      " 0.54 0.54 0.6  0.58 0.52 0.6  0.62 0.6  0.56 0.6  0.5  0.38 0.4  0.58\n",
      " 0.6  0.52 0.44 0.6  0.56 0.58 0.6  0.62 0.5  0.58 0.52 0.52 0.6  0.42\n",
      " 0.52 0.6  0.62 0.6  0.54 0.44 0.4  0.58 0.56 0.6  0.4  0.46 0.46 0.6\n",
      " 0.42 0.6 ]\n",
      "[0.48 0.52 0.54 0.58 0.52 0.48 0.52 0.48 0.46 0.5  0.48 0.56 0.52 0.54\n",
      " 0.5  0.48 0.48 0.5  0.5  0.52 0.58 0.48 0.58 0.48 0.48 0.52 0.5  0.48\n",
      " 0.56 0.5  0.5  0.48 0.5  0.54 0.56 0.56 0.48 0.52 0.56 0.52 0.56 0.5\n",
      " 0.48 0.5  0.56 0.58 0.52 0.54 0.5  0.54 0.56 0.54 0.52 0.48 0.5  0.48\n",
      " 0.5  0.54 0.54 0.58 0.5  0.52 0.54 0.48 0.5  0.5  0.48 0.48 0.5  0.52\n",
      " 0.52 0.52 0.5  0.48 0.5  0.58 0.56 0.56 0.56 0.52 0.48 0.52 0.54 0.52\n",
      " 0.52 0.54 0.52 0.52 0.48 0.58 0.5  0.56 0.54 0.52 0.5  0.46 0.46 0.54\n",
      " 0.52 0.54]\n",
      "[0.58 0.38 0.5  0.6  0.46 0.44 0.4  0.58 0.58 0.58 0.58 0.42 0.38 0.56\n",
      " 0.6  0.48 0.48 0.56 0.48 0.44 0.5  0.54 0.42 0.5  0.6  0.48 0.56 0.6\n",
      " 0.56 0.6  0.46 0.58 0.54 0.52 0.44 0.42 0.54 0.4  0.52 0.4  0.46 0.46\n",
      " 0.56 0.6  0.54 0.46 0.56 0.56 0.44 0.4  0.46 0.54 0.4  0.58 0.56 0.54\n",
      " 0.54 0.38 0.44 0.46 0.56 0.54 0.4  0.58 0.54 0.58 0.46 0.54 0.56 0.4\n",
      " 0.4  0.48 0.5  0.56 0.52 0.46 0.44 0.52 0.42 0.56 0.58 0.48 0.44 0.54\n",
      " 0.54 0.54 0.4  0.54 0.58 0.46 0.52 0.42 0.46 0.42 0.54 0.54 0.58 0.56\n",
      " 0.5  0.46]\n",
      "[0.58 0.4  0.5  0.6  0.44 0.4  0.42 0.58 0.58 0.54 0.58 0.42 0.4  0.56\n",
      " 0.62 0.5  0.44 0.58 0.44 0.42 0.52 0.58 0.44 0.52 0.6  0.52 0.6  0.6\n",
      " 0.6  0.6  0.44 0.58 0.54 0.56 0.5  0.46 0.56 0.42 0.52 0.42 0.48 0.52\n",
      " 0.52 0.6  0.58 0.42 0.56 0.58 0.48 0.42 0.44 0.52 0.42 0.58 0.6  0.56\n",
      " 0.54 0.42 0.4  0.44 0.6  0.56 0.42 0.6  0.56 0.6  0.46 0.52 0.6  0.42\n",
      " 0.42 0.48 0.58 0.56 0.5  0.46 0.46 0.52 0.4  0.54 0.58 0.46 0.44 0.56\n",
      " 0.56 0.56 0.42 0.58 0.6  0.44 0.52 0.42 0.5  0.4  0.56 0.58 0.58 0.58\n",
      " 0.58 0.44]\n",
      "[0.66 0.3  0.48 0.66 0.32 0.3  0.32 0.66 0.64 0.5  0.64 0.32 0.3  0.46\n",
      " 0.66 0.58 0.58 0.6  0.5  0.48 0.46 0.64 0.34 0.46 0.66 0.52 0.64 0.66\n",
      " 0.56 0.68 0.32 0.64 0.54 0.5  0.5  0.32 0.62 0.32 0.5  0.32 0.48 0.46\n",
      " 0.62 0.66 0.64 0.32 0.56 0.56 0.48 0.32 0.5  0.48 0.32 0.64 0.62 0.54\n",
      " 0.6  0.3  0.46 0.46 0.64 0.64 0.32 0.66 0.62 0.6  0.44 0.44 0.48 0.32\n",
      " 0.32 0.48 0.44 0.64 0.58 0.46 0.46 0.56 0.3  0.66 0.64 0.48 0.5  0.48\n",
      " 0.5  0.64 0.32 0.64 0.64 0.38 0.48 0.4  0.46 0.48 0.5  0.64 0.64 0.66\n",
      " 0.48 0.48]\n",
      "[0.68 0.32 0.42 0.7  0.34 0.32 0.32 0.68 0.68 0.5  0.68 0.32 0.36 0.5\n",
      " 0.7  0.68 0.64 0.5  0.6  0.5  0.54 0.66 0.38 0.36 0.7  0.64 0.66 0.68\n",
      " 0.58 0.7  0.32 0.68 0.64 0.64 0.5  0.32 0.68 0.32 0.4  0.32 0.48 0.42\n",
      " 0.66 0.7  0.66 0.32 0.56 0.66 0.48 0.34 0.64 0.42 0.32 0.68 0.66 0.48\n",
      " 0.6  0.32 0.54 0.5  0.66 0.68 0.34 0.68 0.66 0.66 0.36 0.48 0.5  0.32\n",
      " 0.34 0.46 0.48 0.66 0.64 0.5  0.5  0.68 0.36 0.68 0.68 0.42 0.64 0.36\n",
      " 0.5  0.68 0.32 0.68 0.68 0.36 0.48 0.42 0.5  0.48 0.5  0.62 0.6  0.7\n",
      " 0.5  0.62]\n",
      "[(12, 0.6841999999999999), (4, 0.6766), (11, 0.6434), (9, 0.5746), (1, 0.5489999999999999), (7, 0.5384), (10, 0.5378), (2, 0.507), (5, 0.5055999999999999), (3, 0.43760000000000004), (0, 0.424), (6, 0.33340000000000003), (8, 0.25300000000000006)]\n",
      "[0.48 0.52 0.54 0.58 0.52 0.48 0.52 0.48 0.46 0.5  0.48 0.56 0.52 0.54\n",
      " 0.5  0.48 0.48 0.5  0.5  0.52 0.58 0.48 0.58 0.48 0.48 0.52 0.5  0.48\n",
      " 0.56 0.5  0.5  0.48 0.5  0.54 0.56 0.56 0.48 0.52 0.56 0.52 0.56 0.5\n",
      " 0.48 0.5  0.56 0.58 0.52 0.54 0.5  0.54 0.56 0.54 0.52 0.48 0.5  0.48\n",
      " 0.5  0.54 0.54 0.58 0.5  0.52 0.54 0.48 0.5  0.5  0.48 0.48 0.5  0.52\n",
      " 0.52 0.52 0.5  0.48 0.5  0.58 0.56 0.56 0.56 0.52 0.48 0.52 0.54 0.52\n",
      " 0.52 0.54 0.52 0.52 0.48 0.58 0.5  0.56 0.54 0.52 0.5  0.46 0.46 0.54\n",
      " 0.52 0.54]\n",
      "[0.62 0.42 0.44 0.64 0.4  0.38 0.4  0.62 0.62 0.48 0.62 0.5  0.42 0.64\n",
      " 0.64 0.62 0.54 0.6  0.58 0.5  0.62 0.62 0.6  0.4  0.64 0.62 0.64 0.62\n",
      " 0.54 0.64 0.42 0.62 0.62 0.64 0.64 0.68 0.62 0.4  0.44 0.4  0.56 0.44\n",
      " 0.52 0.64 0.62 0.4  0.48 0.62 0.58 0.54 0.54 0.42 0.54 0.62 0.64 0.48\n",
      " 0.6  0.6  0.48 0.56 0.64 0.62 0.42 0.62 0.64 0.64 0.44 0.48 0.5  0.42\n",
      " 0.44 0.42 0.62 0.52 0.58 0.58 0.6  0.66 0.48 0.64 0.62 0.4  0.56 0.5\n",
      " 0.66 0.62 0.42 0.62 0.62 0.4  0.62 0.44 0.62 0.48 0.44 0.6  0.6  0.64\n",
      " 0.66 0.48]\n",
      "[0.48 0.52 0.56 0.58 0.54 0.48 0.5  0.5  0.48 0.5  0.5  0.56 0.52 0.56\n",
      " 0.5  0.5  0.5  0.5  0.5  0.52 0.58 0.5  0.58 0.48 0.5  0.52 0.52 0.48\n",
      " 0.56 0.52 0.5  0.46 0.5  0.52 0.56 0.56 0.52 0.52 0.54 0.5  0.56 0.52\n",
      " 0.48 0.5  0.52 0.54 0.5  0.54 0.5  0.54 0.56 0.54 0.52 0.5  0.5  0.48\n",
      " 0.5  0.54 0.54 0.58 0.54 0.52 0.54 0.52 0.54 0.52 0.5  0.48 0.5  0.52\n",
      " 0.52 0.54 0.5  0.5  0.5  0.6  0.56 0.56 0.54 0.54 0.48 0.54 0.54 0.52\n",
      " 0.52 0.56 0.52 0.52 0.48 0.54 0.52 0.56 0.54 0.52 0.5  0.48 0.48 0.56\n",
      " 0.54 0.54]\n",
      "[0.5  0.46 0.44 0.6  0.44 0.42 0.48 0.48 0.58 0.44 0.48 0.6  0.52 0.6\n",
      " 0.6  0.56 0.42 0.44 0.56 0.56 0.6  0.58 0.5  0.42 0.48 0.54 0.58 0.42\n",
      " 0.44 0.52 0.56 0.54 0.58 0.56 0.56 0.6  0.58 0.44 0.44 0.46 0.54 0.44\n",
      " 0.58 0.52 0.54 0.5  0.42 0.56 0.56 0.58 0.44 0.44 0.58 0.56 0.58 0.42\n",
      " 0.48 0.56 0.44 0.54 0.58 0.52 0.46 0.44 0.48 0.6  0.44 0.46 0.44 0.52\n",
      " 0.52 0.44 0.58 0.42 0.54 0.5  0.56 0.58 0.54 0.58 0.44 0.44 0.5  0.46\n",
      " 0.58 0.52 0.56 0.58 0.56 0.44 0.58 0.46 0.56 0.5  0.44 0.42 0.42 0.52\n",
      " 0.58 0.42]\n",
      "[0.48 0.52 0.54 0.58 0.52 0.48 0.52 0.48 0.46 0.5  0.48 0.56 0.52 0.54\n",
      " 0.5  0.48 0.48 0.5  0.5  0.52 0.58 0.46 0.58 0.48 0.5  0.52 0.5  0.48\n",
      " 0.56 0.5  0.5  0.48 0.5  0.54 0.56 0.54 0.48 0.52 0.56 0.52 0.56 0.5\n",
      " 0.48 0.5  0.56 0.56 0.52 0.54 0.5  0.54 0.56 0.54 0.52 0.46 0.5  0.48\n",
      " 0.5  0.54 0.54 0.58 0.5  0.52 0.54 0.48 0.5  0.5  0.48 0.48 0.5  0.52\n",
      " 0.52 0.52 0.5  0.48 0.5  0.58 0.56 0.56 0.56 0.52 0.48 0.52 0.54 0.52\n",
      " 0.52 0.54 0.52 0.52 0.48 0.56 0.5  0.56 0.54 0.52 0.5  0.46 0.46 0.54\n",
      " 0.52 0.54]\n",
      "[0.56 0.56 0.44 0.58 0.48 0.44 0.46 0.5  0.44 0.46 0.56 0.46 0.48 0.46\n",
      " 0.46 0.46 0.56 0.46 0.46 0.54 0.44 0.46 0.46 0.46 0.58 0.52 0.46 0.56\n",
      " 0.48 0.58 0.46 0.42 0.46 0.42 0.42 0.46 0.56 0.46 0.48 0.44 0.44 0.58\n",
      " 0.44 0.58 0.42 0.44 0.46 0.44 0.44 0.46 0.56 0.54 0.46 0.46 0.48 0.54\n",
      " 0.44 0.44 0.56 0.48 0.48 0.56 0.42 0.56 0.56 0.46 0.44 0.44 0.5  0.52\n",
      " 0.48 0.44 0.44 0.56 0.56 0.48 0.46 0.58 0.4  0.58 0.56 0.5  0.56 0.46\n",
      " 0.48 0.56 0.5  0.56 0.46 0.48 0.44 0.46 0.46 0.54 0.56 0.54 0.46 0.58\n",
      " 0.46 0.56]\n",
      "[0.46 0.5  0.48 0.52 0.46 0.46 0.54 0.46 0.5  0.48 0.46 0.52 0.52 0.52\n",
      " 0.52 0.5  0.46 0.48 0.52 0.52 0.52 0.5  0.5  0.46 0.48 0.52 0.52 0.46\n",
      " 0.48 0.48 0.52 0.48 0.5  0.52 0.54 0.52 0.5  0.48 0.48 0.5  0.52 0.48\n",
      " 0.5  0.48 0.52 0.54 0.46 0.52 0.52 0.54 0.48 0.48 0.54 0.52 0.54 0.46\n",
      " 0.46 0.5  0.46 0.52 0.54 0.48 0.48 0.46 0.44 0.52 0.46 0.48 0.48 0.52\n",
      " 0.46 0.48 0.5  0.46 0.52 0.48 0.54 0.56 0.5  0.54 0.46 0.48 0.48 0.48\n",
      " 0.54 0.44 0.54 0.5  0.5  0.48 0.5  0.48 0.52 0.5  0.48 0.46 0.46 0.48\n",
      " 0.54 0.46]\n",
      "[0.48 0.52 0.54 0.62 0.5  0.5  0.52 0.56 0.5  0.5  0.54 0.58 0.54 0.54\n",
      " 0.5  0.6  0.52 0.48 0.54 0.54 0.6  0.5  0.54 0.46 0.56 0.52 0.52 0.5\n",
      " 0.54 0.56 0.46 0.54 0.54 0.54 0.58 0.46 0.58 0.52 0.5  0.52 0.58 0.5\n",
      " 0.48 0.5  0.58 0.48 0.54 0.54 0.5  0.54 0.56 0.54 0.48 0.54 0.5  0.5\n",
      " 0.52 0.52 0.56 0.58 0.5  0.58 0.56 0.58 0.6  0.54 0.48 0.46 0.48 0.52\n",
      " 0.52 0.5  0.5  0.52 0.52 0.6  0.58 0.58 0.52 0.6  0.54 0.52 0.52 0.48\n",
      " 0.52 0.58 0.52 0.58 0.5  0.5  0.5  0.56 0.58 0.54 0.48 0.5  0.48 0.62\n",
      " 0.52 0.56]\n",
      "[0.48 0.52 0.54 0.58 0.52 0.48 0.52 0.48 0.46 0.5  0.48 0.56 0.52 0.54\n",
      " 0.5  0.48 0.48 0.5  0.5  0.52 0.58 0.48 0.58 0.48 0.48 0.52 0.5  0.48\n",
      " 0.56 0.5  0.5  0.48 0.5  0.54 0.56 0.56 0.48 0.52 0.56 0.52 0.56 0.5\n",
      " 0.48 0.5  0.56 0.58 0.52 0.54 0.5  0.54 0.56 0.54 0.52 0.48 0.5  0.48\n",
      " 0.5  0.54 0.54 0.58 0.5  0.52 0.54 0.48 0.5  0.5  0.48 0.48 0.5  0.52\n",
      " 0.52 0.52 0.5  0.48 0.5  0.58 0.56 0.56 0.56 0.52 0.48 0.52 0.54 0.52\n",
      " 0.52 0.54 0.52 0.52 0.48 0.58 0.5  0.56 0.54 0.52 0.5  0.46 0.46 0.54\n",
      " 0.52 0.54]\n",
      "[0.52 0.42 0.52 0.6  0.5  0.48 0.46 0.56 0.54 0.5  0.56 0.5  0.46 0.52\n",
      " 0.54 0.52 0.48 0.54 0.5  0.48 0.54 0.56 0.52 0.5  0.58 0.5  0.56 0.52\n",
      " 0.52 0.58 0.48 0.54 0.5  0.52 0.54 0.52 0.5  0.44 0.52 0.44 0.5  0.52\n",
      " 0.48 0.54 0.56 0.48 0.52 0.56 0.5  0.48 0.52 0.52 0.5  0.58 0.54 0.48\n",
      " 0.52 0.48 0.48 0.5  0.58 0.52 0.44 0.56 0.54 0.52 0.48 0.5  0.52 0.46\n",
      " 0.46 0.52 0.54 0.5  0.5  0.5  0.5  0.52 0.48 0.54 0.54 0.5  0.52 0.52\n",
      " 0.54 0.58 0.48 0.52 0.54 0.52 0.56 0.48 0.5  0.48 0.5  0.52 0.52 0.58\n",
      " 0.56 0.52]\n",
      "[0.48 0.52 0.54 0.58 0.52 0.48 0.52 0.48 0.46 0.5  0.48 0.56 0.52 0.54\n",
      " 0.5  0.48 0.48 0.5  0.5  0.52 0.58 0.46 0.58 0.48 0.5  0.52 0.5  0.48\n",
      " 0.56 0.5  0.5  0.48 0.5  0.54 0.56 0.56 0.5  0.52 0.54 0.52 0.56 0.5\n",
      " 0.48 0.5  0.56 0.56 0.5  0.54 0.5  0.54 0.56 0.54 0.52 0.46 0.5  0.48\n",
      " 0.5  0.52 0.54 0.58 0.5  0.52 0.54 0.48 0.5  0.5  0.48 0.48 0.5  0.52\n",
      " 0.52 0.52 0.5  0.5  0.5  0.58 0.56 0.56 0.56 0.54 0.48 0.52 0.54 0.52\n",
      " 0.52 0.54 0.52 0.52 0.48 0.56 0.5  0.56 0.54 0.52 0.5  0.46 0.46 0.54\n",
      " 0.52 0.54]\n",
      "[0.48 0.5  0.56 0.58 0.54 0.48 0.52 0.5  0.48 0.5  0.5  0.54 0.52 0.54\n",
      " 0.5  0.52 0.5  0.5  0.5  0.52 0.58 0.48 0.58 0.48 0.5  0.52 0.52 0.48\n",
      " 0.56 0.52 0.5  0.48 0.5  0.54 0.56 0.54 0.52 0.52 0.56 0.5  0.56 0.5\n",
      " 0.48 0.5  0.56 0.5  0.5  0.54 0.5  0.52 0.56 0.54 0.52 0.48 0.5  0.48\n",
      " 0.48 0.5  0.54 0.58 0.5  0.52 0.52 0.5  0.52 0.52 0.48 0.48 0.5  0.52\n",
      " 0.52 0.54 0.5  0.5  0.52 0.58 0.56 0.56 0.56 0.52 0.48 0.52 0.54 0.5\n",
      " 0.52 0.56 0.52 0.52 0.48 0.54 0.5  0.54 0.54 0.52 0.5  0.48 0.48 0.56\n",
      " 0.52 0.54]\n",
      "[(1, 0.5489999999999999), (7, 0.5162000000000001), (5, 0.5055999999999999), (10, 0.4964), (3, 0.43760000000000004), (6, 0.33340000000000003), (12, 0.2802), (0, 0.25300000000000006), (9, 0.25300000000000006), (4, 0.2514), (2, 0.2486), (11, 0.24399999999999997)]\n",
      "[0.48 0.52 0.54 0.58 0.52 0.48 0.52 0.48 0.46 0.5  0.48 0.56 0.52 0.54\n",
      " 0.5  0.48 0.48 0.5  0.5  0.52 0.58 0.46 0.58 0.48 0.5  0.52 0.5  0.48\n",
      " 0.56 0.5  0.5  0.48 0.5  0.54 0.56 0.56 0.5  0.52 0.54 0.52 0.56 0.5\n",
      " 0.48 0.5  0.56 0.56 0.5  0.54 0.5  0.54 0.56 0.54 0.52 0.46 0.5  0.48\n",
      " 0.5  0.52 0.54 0.58 0.5  0.52 0.54 0.48 0.5  0.5  0.48 0.48 0.5  0.52\n",
      " 0.52 0.52 0.5  0.5  0.5  0.58 0.56 0.56 0.56 0.54 0.48 0.52 0.54 0.52\n",
      " 0.52 0.54 0.52 0.52 0.48 0.56 0.5  0.56 0.54 0.52 0.5  0.46 0.46 0.54\n",
      " 0.52 0.54]\n",
      "[0.62 0.42 0.44 0.64 0.4  0.38 0.4  0.62 0.62 0.48 0.62 0.5  0.42 0.64\n",
      " 0.64 0.62 0.54 0.6  0.58 0.5  0.62 0.62 0.6  0.4  0.64 0.62 0.64 0.62\n",
      " 0.54 0.64 0.42 0.62 0.62 0.64 0.64 0.68 0.62 0.4  0.44 0.4  0.56 0.44\n",
      " 0.52 0.64 0.62 0.4  0.48 0.62 0.58 0.54 0.54 0.42 0.54 0.62 0.64 0.48\n",
      " 0.6  0.6  0.48 0.56 0.64 0.62 0.42 0.62 0.64 0.64 0.44 0.48 0.5  0.42\n",
      " 0.44 0.42 0.62 0.52 0.58 0.58 0.6  0.66 0.48 0.64 0.62 0.4  0.56 0.5\n",
      " 0.66 0.62 0.42 0.62 0.62 0.4  0.62 0.44 0.62 0.48 0.44 0.6  0.6  0.64\n",
      " 0.66 0.48]\n",
      "[0.48 0.5  0.54 0.58 0.52 0.48 0.5  0.52 0.48 0.5  0.5  0.56 0.5  0.56\n",
      " 0.5  0.5  0.5  0.5  0.5  0.52 0.58 0.5  0.58 0.48 0.5  0.52 0.52 0.48\n",
      " 0.54 0.54 0.5  0.46 0.5  0.52 0.56 0.54 0.52 0.52 0.54 0.5  0.56 0.52\n",
      " 0.48 0.52 0.52 0.54 0.5  0.52 0.5  0.54 0.56 0.56 0.52 0.52 0.5  0.48\n",
      " 0.5  0.54 0.54 0.58 0.54 0.54 0.54 0.52 0.54 0.52 0.5  0.48 0.5  0.52\n",
      " 0.52 0.54 0.5  0.5  0.5  0.58 0.56 0.56 0.54 0.54 0.48 0.54 0.54 0.52\n",
      " 0.52 0.56 0.52 0.52 0.5  0.54 0.52 0.54 0.54 0.52 0.5  0.48 0.48 0.56\n",
      " 0.54 0.54]\n",
      "[0.5  0.46 0.44 0.6  0.44 0.42 0.48 0.48 0.58 0.44 0.48 0.6  0.52 0.6\n",
      " 0.6  0.56 0.42 0.44 0.56 0.56 0.6  0.58 0.5  0.42 0.48 0.54 0.58 0.42\n",
      " 0.44 0.52 0.56 0.54 0.58 0.56 0.56 0.6  0.58 0.44 0.44 0.46 0.54 0.44\n",
      " 0.58 0.52 0.54 0.5  0.42 0.56 0.56 0.58 0.44 0.44 0.58 0.56 0.58 0.42\n",
      " 0.48 0.56 0.44 0.54 0.58 0.52 0.46 0.44 0.48 0.6  0.44 0.46 0.44 0.52\n",
      " 0.52 0.44 0.58 0.42 0.54 0.5  0.56 0.58 0.54 0.58 0.44 0.44 0.5  0.46\n",
      " 0.58 0.52 0.56 0.58 0.56 0.44 0.58 0.46 0.56 0.5  0.44 0.42 0.42 0.52\n",
      " 0.58 0.42]\n",
      "[0.48 0.52 0.54 0.56 0.52 0.48 0.52 0.48 0.46 0.5  0.48 0.56 0.52 0.54\n",
      " 0.5  0.5  0.48 0.5  0.5  0.52 0.58 0.46 0.58 0.48 0.5  0.52 0.5  0.48\n",
      " 0.56 0.5  0.5  0.48 0.5  0.54 0.56 0.56 0.5  0.52 0.56 0.52 0.56 0.5\n",
      " 0.48 0.5  0.54 0.56 0.52 0.54 0.5  0.54 0.56 0.54 0.52 0.46 0.5  0.48\n",
      " 0.5  0.52 0.54 0.58 0.5  0.5  0.54 0.48 0.5  0.5  0.48 0.48 0.5  0.52\n",
      " 0.52 0.52 0.5  0.5  0.5  0.58 0.56 0.56 0.56 0.54 0.48 0.52 0.54 0.52\n",
      " 0.52 0.54 0.52 0.52 0.48 0.56 0.5  0.54 0.54 0.52 0.5  0.46 0.46 0.54\n",
      " 0.52 0.54]\n",
      "[0.56 0.56 0.44 0.58 0.48 0.44 0.46 0.5  0.44 0.46 0.56 0.46 0.48 0.46\n",
      " 0.46 0.46 0.56 0.46 0.46 0.54 0.44 0.46 0.46 0.46 0.58 0.52 0.46 0.56\n",
      " 0.48 0.58 0.46 0.42 0.46 0.42 0.42 0.46 0.56 0.46 0.48 0.44 0.44 0.58\n",
      " 0.44 0.58 0.42 0.44 0.46 0.44 0.44 0.46 0.56 0.54 0.46 0.46 0.48 0.54\n",
      " 0.44 0.44 0.56 0.48 0.48 0.56 0.42 0.56 0.56 0.46 0.44 0.44 0.5  0.52\n",
      " 0.48 0.44 0.44 0.56 0.56 0.48 0.46 0.58 0.4  0.58 0.56 0.5  0.56 0.46\n",
      " 0.48 0.56 0.5  0.56 0.46 0.48 0.44 0.46 0.46 0.54 0.56 0.54 0.46 0.58\n",
      " 0.46 0.56]\n",
      "[0.46 0.5  0.48 0.52 0.46 0.46 0.54 0.46 0.5  0.48 0.46 0.52 0.52 0.52\n",
      " 0.52 0.5  0.46 0.48 0.52 0.52 0.52 0.5  0.5  0.46 0.48 0.52 0.52 0.46\n",
      " 0.48 0.48 0.52 0.48 0.5  0.52 0.54 0.52 0.5  0.48 0.48 0.5  0.52 0.48\n",
      " 0.5  0.48 0.52 0.54 0.46 0.52 0.52 0.54 0.48 0.48 0.54 0.52 0.54 0.46\n",
      " 0.46 0.5  0.46 0.52 0.54 0.48 0.48 0.46 0.44 0.52 0.46 0.48 0.48 0.52\n",
      " 0.46 0.48 0.5  0.46 0.52 0.48 0.54 0.56 0.5  0.54 0.46 0.48 0.48 0.48\n",
      " 0.54 0.44 0.54 0.5  0.5  0.48 0.5  0.48 0.52 0.5  0.48 0.46 0.46 0.48\n",
      " 0.54 0.46]\n",
      "[0.48 0.52 0.54 0.62 0.5  0.5  0.52 0.56 0.52 0.5  0.54 0.56 0.54 0.54\n",
      " 0.5  0.58 0.52 0.5  0.54 0.54 0.6  0.5  0.54 0.46 0.56 0.52 0.52 0.5\n",
      " 0.54 0.56 0.46 0.54 0.54 0.54 0.58 0.46 0.58 0.52 0.5  0.52 0.58 0.5\n",
      " 0.48 0.52 0.58 0.48 0.56 0.54 0.5  0.54 0.56 0.54 0.48 0.54 0.5  0.5\n",
      " 0.52 0.52 0.56 0.58 0.52 0.58 0.56 0.58 0.6  0.54 0.46 0.46 0.48 0.52\n",
      " 0.52 0.5  0.48 0.52 0.52 0.58 0.58 0.58 0.5  0.6  0.54 0.52 0.52 0.5\n",
      " 0.52 0.58 0.52 0.58 0.52 0.5  0.5  0.56 0.58 0.54 0.48 0.5  0.48 0.62\n",
      " 0.52 0.56]\n",
      "[0.48 0.52 0.54 0.58 0.52 0.48 0.52 0.48 0.46 0.5  0.48 0.56 0.52 0.54\n",
      " 0.5  0.48 0.48 0.5  0.5  0.52 0.58 0.46 0.58 0.48 0.5  0.52 0.5  0.48\n",
      " 0.56 0.5  0.5  0.48 0.5  0.54 0.56 0.56 0.5  0.52 0.54 0.52 0.56 0.5\n",
      " 0.48 0.5  0.56 0.56 0.5  0.54 0.5  0.54 0.56 0.54 0.52 0.46 0.5  0.48\n",
      " 0.5  0.52 0.54 0.58 0.5  0.52 0.54 0.48 0.5  0.5  0.48 0.48 0.5  0.52\n",
      " 0.52 0.52 0.5  0.5  0.5  0.58 0.56 0.56 0.56 0.54 0.48 0.52 0.54 0.52\n",
      " 0.52 0.54 0.52 0.52 0.48 0.56 0.5  0.56 0.54 0.52 0.5  0.46 0.46 0.54\n",
      " 0.52 0.54]\n",
      "[0.52 0.42 0.52 0.6  0.5  0.46 0.46 0.56 0.54 0.52 0.56 0.48 0.46 0.52\n",
      " 0.54 0.52 0.48 0.54 0.48 0.48 0.54 0.56 0.52 0.48 0.58 0.5  0.56 0.52\n",
      " 0.52 0.58 0.46 0.54 0.52 0.52 0.54 0.52 0.5  0.44 0.52 0.44 0.5  0.52\n",
      " 0.48 0.54 0.56 0.5  0.52 0.56 0.5  0.48 0.52 0.52 0.5  0.58 0.54 0.48\n",
      " 0.52 0.48 0.48 0.5  0.58 0.52 0.44 0.56 0.54 0.52 0.48 0.5  0.52 0.46\n",
      " 0.46 0.52 0.52 0.5  0.5  0.5  0.5  0.52 0.48 0.54 0.54 0.5  0.52 0.52\n",
      " 0.54 0.58 0.48 0.52 0.56 0.52 0.56 0.48 0.5  0.48 0.52 0.52 0.52 0.58\n",
      " 0.56 0.52]\n",
      "[0.48 0.5  0.54 0.6  0.52 0.48 0.52 0.5  0.48 0.5  0.5  0.54 0.5  0.54\n",
      " 0.5  0.52 0.5  0.5  0.5  0.52 0.58 0.5  0.58 0.48 0.52 0.52 0.52 0.48\n",
      " 0.56 0.52 0.5  0.48 0.5  0.54 0.56 0.54 0.52 0.52 0.54 0.5  0.56 0.5\n",
      " 0.48 0.5  0.56 0.5  0.5  0.54 0.5  0.52 0.56 0.54 0.52 0.5  0.5  0.48\n",
      " 0.48 0.5  0.54 0.58 0.5  0.52 0.52 0.5  0.52 0.52 0.48 0.48 0.5  0.52\n",
      " 0.52 0.54 0.5  0.5  0.52 0.58 0.56 0.56 0.56 0.52 0.48 0.52 0.54 0.5\n",
      " 0.52 0.56 0.52 0.54 0.48 0.54 0.5  0.54 0.54 0.52 0.5  0.48 0.48 0.58\n",
      " 0.54 0.54]\n",
      "[(1, 0.5489999999999999), (7, 0.5092), (5, 0.5055999999999999), (10, 0.4852), (3, 0.43760000000000004), (6, 0.33340000000000003), (12, 0.28380000000000005), (2, 0.252), (4, 0.244), (0, 0.24399999999999997), (9, 0.24399999999999997)]\n",
      "[8, 11]\n",
      "finished Haralick Feature selection\n"
     ]
    }
   ],
   "source": [
    "h = Haralicks(compute_dog=False)\n",
    "h.featureSelector(5,50,data.scans,data.labels,\"value\",True)       #5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dawit/Programs/anaconda3/envs/bach/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "initialization finished\n"
     ]
    }
   ],
   "source": [
    "#data = SimpleShapeData()\n",
    "data = KneeData()\n",
    "data.loadData(from_original_files=True,pickled_db_path=\"pickled/erik_train.pck\",path='/home/dawit/Datalogi/BachelorProjekt/data/erik/train_data/')\n",
    "m = VGGModel()\n",
    "#m.fit(data.getScansAtSlice(three_slices=True),data.labels,\"ishealthy\",True)\n",
    "e = Evaluator(data_obj=data,method_obj=m,load_as_slice=True,three_slices=True)\n",
    "#d = data.vggData(\"shape\",\"triangle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in e.scans:\n",
    "#    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 38, 56, 64, 55],\n",
       " [2.441266769892536e-05,\n",
       "  29.243480682373047,\n",
       "  30.16129493713379,\n",
       "  30.884754180908203,\n",
       "  31.134431838989258])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.query(0,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.extractFromBatch(data.scans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.matrix[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               2508900   \n",
      "=================================================================\n",
      "Total params: 17,223,588\n",
      "Trainable params: 17,223,588\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-2d95c21aa572>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "m.matrix[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1, 1, 1],\n",
       "        [2, 2, 2]],\n",
       "\n",
       "       [[1, 1, 1],\n",
       "        [2, 2, 2]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.array([[1,2],[1,2]])\n",
    "arr = np.expand_dims(arr,axis=2)\n",
    "arr = np.repeat(arr,3,2)\n",
    "\n",
    "arr.shape\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg_model = VGG16(weights='imagenet', include_top=True)\n",
    "vgg_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 224, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dawit/Programs/anaconda3/envs/bach/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.layers import Flatten\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "from keras.layers.core import Flatten, Dense, Dropout\n",
    "\n",
    "import numpy as np\n",
    "#vgg_model = Model(inputs=vgg_model.input, outputs=f0)\n",
    "#vgg_model.predict(d[0][0])\n",
    "\n",
    "\n",
    "vgg_model = VGG16(weights='imagenet', include_top=True)\n",
    "vgg_model.layers.pop()\n",
    "#vgg_model.layers.Add(Dense(2, activation='softmax'))\n",
    "#layer_name = 'fc2'\n",
    "#connected_layer = vgg_model.get_layer(layer_name)\n",
    "#old_out = Flatten()()\n",
    "\n",
    "out = Dense(2, activation='softmax')(vgg_model.output)\n",
    "model = Model(inputs=vgg_model.input,\n",
    "                                 outputs=out)\n",
    "#vgg_model = VGG16()\n",
    "sgd = optimizers.SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#vgg_model.fit(d[0],d[1])\n",
    "#vgg_model.summary()\n",
    "#layer_name = 'fc2'\n",
    "#intermediate_layer_model = Model(inputs=vgg_model.input,\n",
    "#                                 outputs=vgg_model.get_layer(layer_name).output)\n",
    "#image = d[0][0]\n",
    "#image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "#intermediate_output = intermediate_layer_model.predict(image)\n",
    "#intermediate_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.50095177, 0.49904823],\n",
       "       [0.50095177, 0.49904823]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#intermediate_output.flatten().shape\n",
    "image = d[0][0]\n",
    "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "image = np.append(image ,image,axis=0)\n",
    "#model.summary()\n",
    "p = model.predict(image)\n",
    "\n",
    "p.shape\n",
    "p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import numpy as np\n",
    "\n",
    "class VGG16:\n",
    "    def __init__(self):\n",
    "        self.matrix = []\n",
    "        self.model = VGG16(weights='imagenet', include_top=False,classes=2)\n",
    "\n",
    "    def extractFeatures(self,img,feature_\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lib\n",
    "Name\n",
    "Last Modified\n",
    "\n",
    "indices=None):      \n",
    "        if self.compute_dog:\n",
    "            img = mh.dog(img)\n",
    "        features= mh.features.haralick(img).mean(0)\n",
    "        if feature_indices == None:\n",
    "            feature_indices = self.selected_features\n",
    "        retval = [features[i] for i in feature_indices]\n",
    "        return retval\n",
    "    \n",
    "    def distance(self,vec1,vec2):\n",
    "        return scipy.spatial.distance.euclidean(vec1,vec2)\n",
    "    \n",
    "    def extractFromBatch(self,batch,feature_indices=None, pickled_db_path=\"haralickfeatures.pck\"):\n",
    "        self.matrix = []\n",
    "        for i in batch:\n",
    "            self.matrix.append(self.extractFeatures(i,feature_indices))\n",
    "#            print(i.shape)\n",
    "        with open(pickled_db_path,'wb') as fp:   \n",
    "            pickle.dump(self.matrix,fp)      \n",
    "    \n",
    "    def loadFeatures(self,pickled_db_path=\"haralickfeatures.pck\"):\n",
    "        with open(pickled_db_path,'rb') as fp:\n",
    "            self.matrix = pickle.load(fp)\n",
    "        \n",
    "            \n",
    "    def countClassificationRatio(self,label_indices,label_name,label_value,labels):\n",
    "        relevant_count = 0\n",
    "        non_relevant_count = 0 \n",
    "        for index in label_indices:\n",
    "            if labels[index][label_name] == label_value:\n",
    "                relevant_count += 1\n",
    "            else:\n",
    "                non_relevant_count += 1\n",
    "                \n",
    "        return relevant_count / (len(label_indices))\n",
    "    \n",
    "    def featureSelector(self,size,K,scans,labels,relevant_label,label_value):\n",
    "        selection = []\n",
    "        count = len(scans)\n",
    "        \n",
    "\n",
    "        binary_labels = [1 if i[relevant_label]==label_value else 0 for i in labels ]       \n",
    "        best_auc = 0.0\n",
    "        while len(selection) < size:\n",
    "            index_auc = []\n",
    "            for i in range(13):\n",
    "#                print(i)\n",
    "                if i in selection:\n",
    "                    continue\n",
    "                temp_selection = selection + [i]\n",
    "                probabilities = np.zeros((count),dtype=np.float)\n",
    "                #format database to same vector size\n",
    "                self.extractFromBatch(scans,feature_indices=temp_selection)\n",
    "                for m_iter in range(count):\n",
    "                    retrieved_indices,distances = self.query(scans[m_iter],K+1,feature_indices=temp_selection)           \n",
    "                    retrieved_indices = retrieved_indices[1:]           \n",
    "        #            thresh_index = 0\n",
    "        #            threshold = 0.0\n",
    "        #            while threshold < 1.0:\n",
    "                    ratio = self.countClassificationRatio(retrieved_indices,relevant_label,labels[m_iter][relevant_label],labels)\n",
    "                    if not labels[m_iter][relevant_label] == label_value:\n",
    "                        ratio = 1-ratio\n",
    "                    probabilities[m_iter] = ratio\n",
    "                \n",
    "                auc = roc_auc_score(binary_labels,probabilities)   \n",
    "                index_auc.append((i,auc))\n",
    "            index_auc.sort(reverse=True,key=lambda k: k[1])\n",
    "#            print(index_auc)\n",
    "            print(index_auc)\n",
    "            best_feature = index_auc[0][0]\n",
    "            local_best_auc = index_auc[0][1]\n",
    "            if local_best_auc <= best_auc:\n",
    "                break\n",
    "            else:\n",
    "                best_auc = local_best_auc\n",
    "            selection.append(best_feature)\n",
    "            \n",
    "#        selection.sort()\n",
    "        self.selected_features = selection\n",
    "        print(selection)\n",
    "        print(\"finished Haralick Feature selection\")\n",
    "#               \n",
    "        \n",
    "        \n",
    "    def query(self,query_img,k,feature_indices=None):\n",
    "        if self.matrix is []:\n",
    "            print(\"empty feature matrix\")\n",
    "            return []\n",
    "        query_img_features = self.extractFeatures(query_img,feature_indices)\n",
    "        distances_and_indices = []\n",
    "        for index,database_img_features in enumerate(self.matrix):\n",
    "            distance = self.distance(query_img_features,database_img_features)\n",
    "            distances_and_indices.append((distance,index))\n",
    "        distances_and_indices.sort(key=lambda pair:pair[0])\n",
    "        relevant_dist_and_indices = distances_and_indices[:k]\n",
    "        retval_indices = []\n",
    "        retval_dist = []\n",
    "        for i in relevant_dist_and_indices:\n",
    "            dist,index = i\n",
    "            retval_indices.append(index)\n",
    "            retval_dist.append(dist)\n",
    "        return retval_indices,retval_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/home/dawit/Datalogi/BachelorProjekt/data/\"\n",
    "mrnet_train_ab = MRnetData()\n",
    "mrnet_train_ab.loadData(from_original_files=True,pickled_db_path=base_path+\"../jub/lib/mrnet/abnormal_train.pck\",path_scans=base_path+\"extracted_MR/meniscus/train\",\\\n",
    "               path_labels=base_path+\"MRNet-v1.0/train-abnormal.csv\")\n",
    "#mrnet.prepareTrainingData()\n",
    "\n",
    "mrnet_test_ab = MRnetData()\n",
    "mrnet_test_ab.loadData(from_original_files=True,pickled_db_path=base_path+\"../jub/lib/mrnet/abnormal_valid.pck\",path_scans=base_path+\"extracted_MR/meniscus/test\",\\\n",
    "               path_labels=base_path+\"MRNet-v1.0/valid-abnormal.csv\")\n",
    "#mrnet_test_ab.prepareTrainingData()\n",
    "#mrnet_train_acl = mrnet.genSameScansDifferentLabels(path_labels=base_path+\"MRNet-v1.0/train-acl.csv\")\n",
    "#mrnet_train_meniscus = mrnet.genSameScansDifferentLabels(path_labels=base_path+\"MRNet-v1.0/train-meniscus.csv\")\n",
    "\n",
    "#test_data = KneeData()\n",
    "#test_data.loadData(from_original_files=True,pickled_db_path=\"scans_and_labels_test.pck\",path='/home/dawit/Datalogi/BachelorProjekt/data/test_data/')\n",
    "#test_data.prepareTrainingData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "47\n"
     ]
    }
   ],
   "source": [
    "depth = [i.shape[2] for i in mrnet_train_ab.scans]\n",
    "print(min(depth)) #21 17\n",
    "print(max(depth)) #45 46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrnet_test_ab.loadData(from_original_files=True,pickled_db_path=base_path+\"../jub/lib/mrnet/abnormal_train.pck\",path_scans=base_path+\"extracted_MR/abnormal/train\",\\\n",
    "               path_labels=base_path+\"MRNet-v1.0/train-abnormal.csv\")\n",
    "mrnet.prepareTrainingData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dawit/Programs/anaconda3/envs/bach/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/dawit/Datalogi/BachelorProjekt/jub/lib/MLMethod.py:254: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Time usage: 0:00:00\n"
     ]
    }
   ],
   "source": [
    "ml_method = ConvolutionMethod(mrnet_acl,load_prev_model=False,input_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialization finished\n"
     ]
    }
   ],
   "source": [
    "e_ML_shape  = Evaluator(data_obj=mrnet_acl,method_obj=ml_method,load_as_slice=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 69.12520146369934\n",
      "0.36300000000000004\n"
     ]
    }
   ],
   "source": [
    "print(e_ML_shape.MAP(mrnet.count,5,[\"value\"]))\n",
    "#plotter = Plotter()\n",
    "#plotter.plotSingleShape(mrnet.getScansAtSlice()[0])\n",
    "#mrnet.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 256, 32)\n",
      "(256, 256, 32)\n",
      "(256, 256, 36)\n",
      "(256, 256, 20)\n",
      "(256, 256, 30)\n",
      "(256, 256, 38)\n",
      "(256, 256, 26)\n",
      "(256, 256, 36)\n",
      "(256, 256, 26)\n",
      "(256, 256, 26)\n"
     ]
    }
   ],
   "source": [
    "for i in mrnet.scans:\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'valueTrue': [0, 1, 4, 7, 9], 'valueFalse': [2, 3, 5, 6, 8]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrnet.categorizeImages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 256)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrnet.getScansAtSlice()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrnet.prepareTrainingData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[0, 0, 1, ..., 2, 1, 0],\n",
       "         [0, 0, 1, ..., 2, 1, 0],\n",
       "         [0, 0, 4, ..., 1, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 3, ..., 4, 2, 0],\n",
       "         [0, 0, 3, ..., 6, 0, 0],\n",
       "         [0, 0, 3, ..., 5, 1, 0]]], dtype=int32), array([[1., 0.]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrnet.getTrainingData(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
